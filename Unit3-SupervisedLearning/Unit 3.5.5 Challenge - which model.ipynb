{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge: what model can answer this question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a fairly substantial starting toolbox of supervised learning methods that you can use to tackle a host of exciting problems. To make sure all of these ideas are organized in your mind, please go through the list of problems below. For each, identify which supervised learning method(s) would be best for addressing that particular problem. Explain your reasoning and discuss your answers with your mentor.\n",
    "\n",
    "1. Predict the running times of prospective Olympic sprinters using data from the last 20 Olympics.\n",
    "    - Regression, \n",
    "    - Possibilities: linear regression\n",
    "    - relationships between features are probably not complex, so linear regression fits well.\n",
    "2. You have more features (columns) than rows in your dataset.\n",
    "    - Possibilities: naive bayes, maybe KNN \n",
    "3. Identify the most important characteristic predicting likelihood of being jailed before age 20.\n",
    "    - Classifier\n",
    "    - Possibilities: logistic lasso regression, random forest (feature importance)\n",
    "    - Probably not: logistic regression, naive bayes\n",
    "4. Implement a filter to “highlight” emails that might be important to the recipient\n",
    "    - Classifier\n",
    "    - Possibilities: Naive Bayes\n",
    "    - Probably not: linear regression\n",
    "5. You have 1000+ features.\n",
    "    - Possibilities: random forest, lasso/ridge regression (w/ feature reduction), naive bayes, SVM\n",
    "    - Probably not: KNN, OLS regression\n",
    "6. Predict whether someone who adds items to their cart on a website will purchase the items.\n",
    "    - Classifier\n",
    "    - Possibilities: KNN, logistic regression, random forest, SVM\n",
    "    - Probably not: linear regression,\n",
    "7. Your dataset dimensions are 982400 x 500\n",
    "    - Possibilities: naive bayes, lasso/ridge regression (w/ feature reduction), random forest\n",
    "    - Probably not: SVM, KNN, OLS regression\n",
    "8. Identify faces in an image.\n",
    "    - Classifier. Can supervised learning do this?\n",
    "    - Possiblities: KNN, PCA and then KNN\n",
    "9. Predict which of three flavors of ice cream will be most popular with boys vs girls.\n",
    "    - Classifier\n",
    "    - Possibilities: naive bayes, KNN,\n",
    "    - Probably not: linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model types to choose from:\n",
    "- linear regression\n",
    "    - pros: explainability, fast, relatively little data needed\n",
    "    - cons: poor with complex relationships, \n",
    "- logistic regression\n",
    "    - pros: can be used for classification/probabilities\n",
    "- lasso regression (linear and logistic)\n",
    "    - pros: good for eliminating weak predictors/finding strongest predictors (feature selection), combats overfitting (multicollinearity)\n",
    "    - cons: can arbitrarily drop predictors when collinearity exists\n",
    "- ridge regression (linear and logistic)\n",
    "    - pros: good for large amounts of data, combats overfitting ((multicollinearity)/smaller variance \n",
    "    - cons: not parsimonious (doesn't reduce variables), \n",
    "- naive bayes\n",
    "    - classification\n",
    "    - pros: -simple, fast, sentiment classification, little data needed, can be trained on large amounts of data\n",
    "    - cons: assumption of independence\n",
    "- svm\n",
    "    - regression and classification\n",
    "    - pro: little data needed, accuracy on small clean datasets\n",
    "    - cons: high training time, Less effective on noisier datasets with overlapping classes\n",
    "- random forest/decision tree\n",
    "    - regression and classification\n",
    "    - pros: good with complex/non-linear relationships, strong performer\n",
    "    - cons: can get large + slow, \n",
    "- nearest neighbors\n",
    "    - regression and classification\n",
    "    - pros: no assumptions needed, easy to understand, Flexible to feature / distance choices\n",
    "    - cons: regression can't predict values outside of training data, computationally expensive (slow prediction, high memory usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model attributes to consider: computation cost, prediction accuracy, explainability, identifying important features/number of , regression/classification, assumptions/complex data, data required, sensitivity to outliers, tendency to overfit (bias vs variance),"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
