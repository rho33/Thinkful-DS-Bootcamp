{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this challenge, you will need to choose a corpus of data from nltk or another source that includes categories you can predict and create an analysis pipeline that includes the following steps:\n",
    "\n",
    "1. Data cleaning / processing / language parsing\n",
    "2. Create features using two different NLP methods: For example, BoW vs tf-idf.\n",
    "3. Use the features to fit supervised learning models for each feature set to predict the category outcomes.\n",
    "4. Assess your models using cross-validation and determine whether one model performed better.\n",
    "5. Pick one of the models and try to increase accuracy by at least 5 percentage points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:44:15.975865Z",
     "start_time": "2019-01-07T05:44:15.951880Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import gutenberg, stopwords\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:44:16.150800Z",
     "start_time": "2019-01-07T05:44:15.978865Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    # Visual inspection identifies a form of punctuation spaCy does not\n",
    "    # recognize: the double dash '--'.  Better get rid of it now!\n",
    "    text = re.sub(r'--',' ',text)\n",
    "    text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:44:16.314927Z",
     "start_time": "2019-01-07T05:44:16.155796Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def bag_of_words(text):\n",
    "    '''Filter out punctuation and stop words.'''\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:44:16.412929Z",
     "start_time": "2019-01-07T05:44:16.318905Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def bow_features(sentences, common_words):\n",
    "    '''Creates a data frame with features for each word in our common word set.\n",
    "    Each value is the count of the times the word appears in each sentence.'''\n",
    "    counts = np.zeros((len(sentences),len(common_words)), dtype=int)\n",
    "   \n",
    "    # Process each row, counting the occurrence of words in each sentence.\n",
    "    text_sentences = list(sentences[0])\n",
    "    for i, sentence in enumerate(text_sentences):\n",
    "        \n",
    "        # Convert the sentence to lemmas, then filter out punctuation,\n",
    "        # stop words, and uncommon words.\n",
    "        words = [token.lemma_\n",
    "                 for token in sentence\n",
    "                 if (\n",
    "                     not token.is_punct\n",
    "                     and not token.is_stop\n",
    "                     and token.lemma_ in common_words\n",
    "                 )]\n",
    "        \n",
    "        # Populate the row with word counts.\n",
    "        for word in words:\n",
    "            counts[i, common_words.index(word)] += 1\n",
    "        \n",
    "        # This counter is just to make sure the kernel didn't hang.\n",
    "#         if i % 1000 == 0:\n",
    "#             print(\"Processing row {}\".format(i))\n",
    "        \n",
    "    counts_df = pd.DataFrame(data=counts, columns=common_words)\n",
    "    counts_df['text_sentence'] = sentences[0]\n",
    "    counts_df['text_source'] = sentences[1]\n",
    "    \n",
    "    return counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:44:16.537575Z",
     "start_time": "2019-01-07T05:44:16.415929Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def make_bow_df(texts, authors):\n",
    "    cleaned_texts = [text_cleaner(text) for text in texts]\n",
    "    \n",
    "    nlp = spacy.load('en')\n",
    "    docs = [nlp(cleaned) for cleaned in cleaned_texts]\n",
    "    \n",
    "    bags = [bag_of_words(doc) for doc in docs]\n",
    "    common_words = list({word for bag in bags for word in bag})\n",
    "    \n",
    "    sentences = pd.DataFrame([[sent, authors[i]] for i, doc in enumerate(docs) for sent in doc.sents])\n",
    "    \n",
    "    counts_df = bow_features(sentences, common_words)\n",
    "    \n",
    "    return counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:44:16.862373Z",
     "start_time": "2019-01-07T05:44:16.539549Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def tfidf_features(sentences):\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, \n",
    "                             min_df=5, \n",
    "                             stop_words='english',   \n",
    "                             use_idf=True,\n",
    "                             norm=u'l2', \n",
    "                             smooth_idf=True \n",
    "                            )\n",
    "    text_sentences = [str(sent) for sent in sentences[0]]\n",
    "    tfidf = vectorizer.fit_transform(text_sentences).tocsr()\n",
    "    \n",
    "    df = pd.DataFrame(tfidf.todense(), columns=vectorizer.get_feature_names())\n",
    "    df['text_sentence'] = sentences[0]\n",
    "    df['text_source'] = sentences[1]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:44:16.972121Z",
     "start_time": "2019-01-07T05:44:16.865372Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "def make_tfidf_df(texts, authors):\n",
    "    cleaned_texts = [text_cleaner(text) for text in texts]\n",
    "    \n",
    "    nlp = spacy.load('en')\n",
    "    docs = [nlp(cleaned) for cleaned in cleaned_texts]\n",
    "    \n",
    "    sentences = pd.DataFrame([[sent, authors[i]] for i, doc in enumerate(docs) for sent in doc.sents])\n",
    "    \n",
    "    df = tfidf_features(sentences)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:44:17.070806Z",
     "start_time": "2019-01-07T05:44:16.975108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:44:17.184236Z",
     "start_time": "2019-01-07T05:44:17.074804Z"
    }
   },
   "outputs": [],
   "source": [
    "# lets pick caesar, thursday, and stories\n",
    "caesar = gutenberg.raw('shakespeare-caesar.txt')\n",
    "thursday = gutenberg.raw('chesterton-thursday.txt')\n",
    "stories = gutenberg.raw('bryant-stories.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:44:17.290248Z",
     "start_time": "2019-01-07T05:44:17.186168Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = [caesar, thursday, stories]\n",
    "authors = ['Shakespeare', 'Chesterton', 'Bryant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:45:20.686445Z",
     "start_time": "2019-01-07T05:44:17.293228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fig</th>\n",
       "      <th>italy</th>\n",
       "      <th>appoint</th>\n",
       "      <th>fashter</th>\n",
       "      <th>son</th>\n",
       "      <th>swiftly</th>\n",
       "      <th>darknesse</th>\n",
       "      <th>morrow</th>\n",
       "      <th>iust</th>\n",
       "      <th>utterly</th>\n",
       "      <th>...</th>\n",
       "      <th>tragic</th>\n",
       "      <th>roman</th>\n",
       "      <th>husband</th>\n",
       "      <th>however</th>\n",
       "      <th>adventures</th>\n",
       "      <th>love</th>\n",
       "      <th>breath</th>\n",
       "      <th>railway</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Actus, Primus, .)</td>\n",
       "      <td>Shakespeare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Scoena, Prima, .)</td>\n",
       "      <td>Shakespeare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Enter, Flauius, ,, Murellus, ,, and, certaine...</td>\n",
       "      <td>Shakespeare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Flauius, .)</td>\n",
       "      <td>Shakespeare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Hence, :, home, you, idle, Creatures, ,, get,...</td>\n",
       "      <td>Shakespeare</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4460 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   fig  italy  appoint  fashter  son  swiftly  darknesse  morrow  iust  \\\n",
       "0    0      0        0        0    0        0          0       0     0   \n",
       "1    0      0        0        0    0        0          0       0     0   \n",
       "2    0      0        0        0    0        0          0       0     0   \n",
       "3    0      0        0        0    0        0          0       0     0   \n",
       "4    0      0        0        0    0        0          0       0     0   \n",
       "\n",
       "   utterly     ...       tragic  roman  husband  however  adventures  love  \\\n",
       "0        0     ...            0      0        0        0           0     0   \n",
       "1        0     ...            0      0        0        0           0     0   \n",
       "2        0     ...            0      0        0        0           0     0   \n",
       "3        0     ...            0      0        0        0           0     0   \n",
       "4        0     ...            0      0        0        0           0     0   \n",
       "\n",
       "   breath  railway                                      text_sentence  \\\n",
       "0       0        0                                 (Actus, Primus, .)   \n",
       "1       0        0                                 (Scoena, Prima, .)   \n",
       "2       0        0  (Enter, Flauius, ,, Murellus, ,, and, certaine...   \n",
       "3       0        0                                       (Flauius, .)   \n",
       "4       0        0  (Hence, :, home, you, idle, Creatures, ,, get,...   \n",
       "\n",
       "   text_source  \n",
       "0  Shakespeare  \n",
       "1  Shakespeare  \n",
       "2  Shakespeare  \n",
       "3  Shakespeare  \n",
       "4  Shakespeare  \n",
       "\n",
       "[5 rows x 4460 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_df = make_bow_df(texts, authors)\n",
    "bow_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:46:11.604418Z",
     "start_time": "2019-01-07T05:45:20.691440Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>abrupt</th>\n",
       "      <th>abruptly</th>\n",
       "      <th>absent</th>\n",
       "      <th>absurd</th>\n",
       "      <th>accent</th>\n",
       "      <th>accept</th>\n",
       "      <th>accident</th>\n",
       "      <th>acorn</th>\n",
       "      <th>act</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yong</th>\n",
       "      <th>young</th>\n",
       "      <th>zelia</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(Actus, Primus, .)</td>\n",
       "      <td>Shakespeare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(Scoena, Prima, .)</td>\n",
       "      <td>Shakespeare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(Enter, Flauius, ,, Murellus, ,, and, certaine...</td>\n",
       "      <td>Shakespeare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(Flauius, .)</td>\n",
       "      <td>Shakespeare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>(Hence, :, home, you, idle, Creatures, ,, get,...</td>\n",
       "      <td>Shakespeare</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   able  abrupt  abruptly  absent  absurd  accent  accept  accident  acorn  \\\n",
       "0   0.0     0.0       0.0     0.0     0.0     0.0     0.0       0.0    0.0   \n",
       "1   0.0     0.0       0.0     0.0     0.0     0.0     0.0       0.0    0.0   \n",
       "2   0.0     0.0       0.0     0.0     0.0     0.0     0.0       0.0    0.0   \n",
       "3   0.0     0.0       0.0     0.0     0.0     0.0     0.0       0.0    0.0   \n",
       "4   0.0     0.0       0.0     0.0     0.0     0.0     0.0       0.0    0.0   \n",
       "\n",
       "   act     ...       year  years  yellow  yes  yesterday  yong  young  zelia  \\\n",
       "0  0.0     ...        0.0    0.0     0.0  0.0        0.0   0.0    0.0    0.0   \n",
       "1  0.0     ...        0.0    0.0     0.0  0.0        0.0   0.0    0.0    0.0   \n",
       "2  0.0     ...        0.0    0.0     0.0  0.0        0.0   0.0    0.0    0.0   \n",
       "3  0.0     ...        0.0    0.0     0.0  0.0        0.0   0.0    0.0    0.0   \n",
       "4  0.0     ...        0.0    0.0     0.0  0.0        0.0   0.0    0.0    0.0   \n",
       "\n",
       "                                       text_sentence  text_source  \n",
       "0                                 (Actus, Primus, .)  Shakespeare  \n",
       "1                                 (Scoena, Prima, .)  Shakespeare  \n",
       "2  (Enter, Flauius, ,, Murellus, ,, and, certaine...  Shakespeare  \n",
       "3                                       (Flauius, .)  Shakespeare  \n",
       "4  (Hence, :, home, you, idle, Creatures, ,, get,...  Shakespeare  \n",
       "\n",
       "[5 rows x 2139 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = make_tfidf_df(texts, authors)\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:46:12.803716Z",
     "start_time": "2019-01-07T05:46:11.611411Z"
    }
   },
   "outputs": [],
   "source": [
    "X_bow = np.array(bow_df.drop(['text_sentence','text_source'], 1))\n",
    "y_bow = bow_df['text_source']\n",
    "\n",
    "X_tfidf = np.array(tfidf_df.drop(['text_sentence','text_source'], 1))\n",
    "y_tfidf = tfidf_df['text_source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:46:16.357713Z",
     "start_time": "2019-01-07T05:46:12.813710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8428004197839233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.84506041, 0.85704125, 0.87250712, 0.85327635, 0.83618234,\n",
       "       0.79273504])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Words\n",
    "model = MultinomialNB()\n",
    "cv = cross_val_score(model, X_bow, y_bow, cv=6)\n",
    "print(cv.mean())\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:46:18.070215Z",
     "start_time": "2019-01-07T05:46:16.369705Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8051999666623915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.78464819, 0.82076814, 0.85754986, 0.81125356, 0.8048433 ,\n",
       "       0.75213675])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf\n",
    "model = MultinomialNB()\n",
    "cv = cross_val_score(model, X_tfidf, y_tfidf, cv=6)\n",
    "print(cv.mean())\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:46:22.003998Z",
     "start_time": "2019-01-07T05:46:18.074213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8069751964096034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.81023454, 0.80512091, 0.84045584, 0.81766382, 0.8048433 ,\n",
       "       0.76353276])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Words\n",
    "model = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "cv = cross_val_score(model, X_bow, y_bow, cv=6)\n",
    "print(cv.mean())\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:46:23.648261Z",
     "start_time": "2019-01-07T05:46:22.008993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7836014432636585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.77754087, 0.80440967, 0.83974359, 0.77849003, 0.77706553,\n",
       "       0.72435897])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Words\n",
    "model = LogisticRegression(solver='liblinear', multi_class='auto')\n",
    "cv = cross_val_score(model, X_tfidf, y_tfidf, cv=6)\n",
    "print(cv.mean())\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:46:46.986204Z",
     "start_time": "2019-01-07T05:46:23.653258Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7127735155687499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.71926084, 0.71906117, 0.75213675, 0.71438746, 0.71509972,\n",
       "       0.65669516])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bag of words\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "cv = cross_val_score(model, X_bow, y_bow, cv=6)\n",
    "print(cv.mean())\n",
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:09.622498Z",
     "start_time": "2019-01-07T05:46:46.990201Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.727725463782023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.72850036, 0.72759602, 0.77136752, 0.74358974, 0.71438746,\n",
       "       0.68091168])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tfidf\n",
    "model = RandomForestClassifier(n_estimators=10)\n",
    "cv = cross_val_score(model, X_tfidf, y_tfidf, cv=6)\n",
    "print(cv.mean())\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve Bag of Words Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:47:09.636489Z",
     "start_time": "2019-01-07T05:47:09.627497Z"
    }
   },
   "outputs": [],
   "source": [
    "# rewrite bag of words function to make a bigger bag of words\n",
    "def bag_of_words(text):\n",
    "    '''Filter out punctuation and stop words.'''\n",
    "    allwords = [token.lemma_\n",
    "                for token in text\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    \n",
    "    # Return the most common words.\n",
    "    return [item[0] for item in Counter(allwords).most_common(16000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:48:17.008688Z",
     "start_time": "2019-01-07T05:47:09.639487Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>exception</th>\n",
       "      <th>italy</th>\n",
       "      <th>six</th>\n",
       "      <th>gild</th>\n",
       "      <th>locust</th>\n",
       "      <th>waye</th>\n",
       "      <th>rhinoceros</th>\n",
       "      <th>valuable</th>\n",
       "      <th>india</th>\n",
       "      <th>outspread</th>\n",
       "      <th>...</th>\n",
       "      <th>loiterer</th>\n",
       "      <th>shtick</th>\n",
       "      <th>somersault</th>\n",
       "      <th>sweep</th>\n",
       "      <th>vouchsafe</th>\n",
       "      <th>clatter</th>\n",
       "      <th>however</th>\n",
       "      <th>breath</th>\n",
       "      <th>text_sentence</th>\n",
       "      <th>text_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Actus, Primus, .)</td>\n",
       "      <td>Shakespeare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Scoena, Prima, .)</td>\n",
       "      <td>Shakespeare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Enter, Flauius, ,, Murellus, ,, and, certaine...</td>\n",
       "      <td>Shakespeare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Flauius, .)</td>\n",
       "      <td>Shakespeare</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>(Hence, :, home, you, idle, Creatures, ,, get,...</td>\n",
       "      <td>Shakespeare</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 7905 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   exception  italy  six  gild  locust  waye  rhinoceros  valuable  india  \\\n",
       "0          0      0    0     0       0     0           0         0      0   \n",
       "1          0      0    0     0       0     0           0         0      0   \n",
       "2          0      0    0     0       0     0           0         0      0   \n",
       "3          0      0    0     0       0     0           0         0      0   \n",
       "4          0      0    0     0       0     0           0         0      0   \n",
       "\n",
       "   outspread     ...       loiterer  shtick  somersault  sweep  vouchsafe  \\\n",
       "0          0     ...              0       0           0      0          0   \n",
       "1          0     ...              0       0           0      0          0   \n",
       "2          0     ...              0       0           0      0          0   \n",
       "3          0     ...              0       0           0      0          0   \n",
       "4          0     ...              0       0           0      0          0   \n",
       "\n",
       "   clatter  however  breath  \\\n",
       "0        0        0       0   \n",
       "1        0        0       0   \n",
       "2        0        0       0   \n",
       "3        0        0       0   \n",
       "4        0        0       0   \n",
       "\n",
       "                                       text_sentence  text_source  \n",
       "0                                 (Actus, Primus, .)  Shakespeare  \n",
       "1                                 (Scoena, Prima, .)  Shakespeare  \n",
       "2  (Enter, Flauius, ,, Murellus, ,, and, certaine...  Shakespeare  \n",
       "3                                       (Flauius, .)  Shakespeare  \n",
       "4  (Hence, :, home, you, idle, Creatures, ,, get,...  Shakespeare  \n",
       "\n",
       "[5 rows x 7905 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_bow_df = make_bow_df(texts, authors)\n",
    "big_bow_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:48:18.039107Z",
     "start_time": "2019-01-07T05:48:17.014683Z"
    }
   },
   "outputs": [],
   "source": [
    "X_big_bow = np.array(big_bow_df.drop(['text_sentence','text_source'], 1))\n",
    "y_big_bow = big_bow_df['text_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:48:26.185294Z",
     "start_time": "2019-01-07T05:48:18.046093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8385342571320576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.83013504, 0.8485064 , 0.86965812, 0.85683761, 0.82977208,\n",
       "       0.7962963 ])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Words\n",
    "model = MultinomialNB()\n",
    "cv = cross_val_score(model, X_big_bow, y_big_bow, cv=6)\n",
    "print(cv.mean())\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use length of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:48:26.207281Z",
     "start_time": "2019-01-07T05:48:26.190292Z"
    }
   },
   "outputs": [],
   "source": [
    "bow_df['sentence_length'] = bow_df['text_sentence'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:48:26.748971Z",
     "start_time": "2019-01-07T05:48:26.354197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakespeare mean sent length: 11.727611940298507\n",
      "Chesterton mean sent length: 19.92470051340559\n",
      "Bryant mean sent length: 20.203670385030588\n"
     ]
    }
   ],
   "source": [
    "print('Shakespeare mean sent length:', bow_df[bow_df['text_source']=='Shakespeare']['sentence_length'].mean())\n",
    "print('Chesterton mean sent length:', bow_df[bow_df['text_source']=='Chesterton']['sentence_length'].mean())\n",
    "print('Bryant mean sent length:', bow_df[bow_df['text_source']=='Bryant']['sentence_length'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:48:26.773956Z",
     "start_time": "2019-01-07T05:48:26.756968Z"
    }
   },
   "outputs": [],
   "source": [
    "bow_df['long_sentence'] = bow_df['sentence_length'].apply(lambda x: x//7)\n",
    "# big_bow_df['short_sentence'] = big_bow_df['sentence_length'].apply(lambda x: x <= 17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:48:28.314081Z",
     "start_time": "2019-01-07T05:48:26.776956Z"
    }
   },
   "outputs": [],
   "source": [
    "X_bow = np.array(bow_df.drop(['text_sentence','text_source', 'sentence_length'], 1))\n",
    "y_bow = bow_df['text_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:48:40.171989Z",
     "start_time": "2019-01-07T05:48:28.319085Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8452917771783968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.84363895, 0.8655761 , 0.87250712, 0.85683761, 0.83333333,\n",
       "       0.79985755])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Words\n",
    "model = MultinomialNB(fit_prior=False)\n",
    "cv = cross_val_score(model, X_bow, y_bow, cv=6)\n",
    "print(cv.mean())\n",
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Rounded Avg Word Length and Length of Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:48:40.488863Z",
     "start_time": "2019-01-07T05:48:40.174984Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "def avg_word_length(sentence):\n",
    "    words = [token\n",
    "                for token in sentence\n",
    "                if not token.is_punct\n",
    "                and not token.is_stop]\n",
    "    lengths = np.array([len(word) for word in words])\n",
    "    return round(np.mean(lengths), 0)\n",
    "\n",
    "bow_df['avg_word_length'] = bow_df['text_sentence'].apply(avg_word_length)\n",
    "bow_df['avg_word_length'] = bow_df['avg_word_length'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:48:40.705680Z",
     "start_time": "2019-01-07T05:48:40.491803Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakespeare sentences mean avg word length: 4.627798507462686\n",
      "Chesterton sentences mean avg word length: 5.276098117512835\n",
      "Bryant sentences mean avg word length: 4.7430730478589425\n"
     ]
    }
   ],
   "source": [
    "print('Shakespeare sentences mean avg word length:', bow_df[bow_df['text_source']=='Shakespeare']['avg_word_length'].mean())\n",
    "print('Chesterton sentences mean avg word length:', bow_df[bow_df['text_source']=='Chesterton']['avg_word_length'].mean())\n",
    "print('Bryant sentences mean avg word length:', bow_df[bow_df['text_source']=='Bryant']['avg_word_length'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:48:41.733094Z",
     "start_time": "2019-01-07T05:48:40.708678Z"
    }
   },
   "outputs": [],
   "source": [
    "X_bow = np.array(bow_df.drop(['text_sentence','text_source', 'sentence_length'], 1))\n",
    "y_bow = bow_df['text_source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-07T05:48:52.369011Z",
     "start_time": "2019-01-07T05:48:41.736090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8419711474175994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.83795309, 0.86059744, 0.86965812, 0.85968661, 0.82834758,\n",
       "       0.79558405])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Words\n",
    "model = MultinomialNB(fit_prior=False)\n",
    "cv = cross_val_score(model, X_bow, y_bow, cv=6)\n",
    "print(cv.mean())\n",
    "cv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
